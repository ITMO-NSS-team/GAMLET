training_method: train


batch_size: 1024

model:
  name: "HomogeneousGCN"
  model_parameters:
    in_channels: 15
    out_channels: 1
    gnn_hidden_channels: 8
    gnn_num_layers: 3
    mlp_hidden_channels: 8
    mlp_num_layers: 2
    aggregation: "sum"
    clip_output: null
  loss_name: "mse_loss"
  lr: 0.001

train_dataset:
  name: "SyntheticPipelineDataset"
  parameters:
    root: "C:\\Users\\Konstantin\\PycharmProjects\\NIR\\dataset\\synthetic_pipeline_dataset"
    split: "train"
    direction: "reversed"
    dataset_len: 5000
    num_pipeline_node_types: 15
    max_pipeline_length: 9

val_dataset:
  name: "SyntheticPipelineDataset"
  parameters:
    root: "C:\\Users\\Konstantin\\PycharmProjects\\NIR\\dataset\\synthetic_pipeline_dataset"
    split: "val"
    direction: "reversed"
    dataset_len: 5000
    num_pipeline_node_types: 15
    max_pipeline_length: 9

test_dataset:
  name: "SyntheticPipelineDataset"
  parameters:
    root: "C:\\Users\\Konstantin\\PycharmProjects\\NIR\\dataset\\synthetic_pipeline_dataset"
    split: "test"
    direction: "reversed"
    dataset_len: 5000
    num_pipeline_node_types: 15
    max_pipeline_length: 9

trainer:
  max_epochs: 1000
  devices: "auto"
  log_every_n_steps: 4
  check_val_every_n_epoch: 1

tensorboard_logger:
  save_dir: "C:\\Users\\Konstantin\\PycharmProjects\\meta-automl-research\\experiments\\homogeneous_gcn\\synthetic_pipeline_dataset"
  name: "test_run"

model_checkpoint_callback:
  save_top_k: 1
  monitor: "val_loss"
  save_last: True
  every_n_epochs: 1