training_method: "train_hetero_surrogate_model"
batch_size: 1024

num_dataloader_workers: 0
model:
  class: "HeteroPipelineRankingSurrogateModel"
  model_parameters:
    pipeline_extractor:
      operation_encoding: "ordinal"
      hyperparameters_embedder: null
    node_embedder:
      op_hyperparams_embedder:
        class: HyperparametersEmbedder #"PretrainedHyperparametersEmbedder"
        # autoencoder_ckpt_path: "/Users/cherniak/itmo_job/GAMLET/experiment_logs/embed_hyperparameters/to_8_with_learnables/checkpoints/epoch=9-step=980.ckpt"
        out_dim: 8
        # trainable: true
      # op_name_embedder:
      #   class: NameEmbedder
      #   out_dim: 2
      # embedding_joiner:
      #   class: CatEmbeddingJoiner
    pipeline_encoder:
      type: "simple_graph_encoder"  # graph_transformer
      d_model: 64
      in_size: 8 # model.node_embedder.op_hyperparams_embedder.out_dim
      num_heads: 8
      num_layers: 2
      dropout: 0.3
      in_embed: false
      batch_norm: True  # If false, LayerNorm is used.
      gnn_type: "graphsage"
      dim_feedforward: null  # The parameter is inferred
      num_class: 1
      k_hop: 2
      se: "gnn"
      deg: null  # The parameter is inferred
      global_pool: "mean"
      use_edge_attr: False

  weight_decay: 0.0001
  lr: 0.001
  temperature: 5

dataset_params:
  root_path: "/home/cherniak/itmo_job/GAMLET/data/no_meta_features_and_fedot_pipelines_raw"
  encode_type: null

trainer:
  log_every_n_steps: 1
  num_sanity_val_steps: 0
  max_epochs: 15
  accelerator: "cuda"
  devices: "auto"

tensorboard_logger:
  save_dir: "/home/cherniak/itmo_job/GAMLET/experiment_logs/"
  name: "no_meta_features_and_fedot_pipelines_(type_and_hparams)/"

model_checkpoint_callback:
  save_top_k: 1
  monitor: "val_ndcg"
  mode: "max"
  save_last: False
  every_n_epochs: 1

early_stopping_callback:
  monitor: "val_ndcg"
  mode: "max"
  patience: 10
