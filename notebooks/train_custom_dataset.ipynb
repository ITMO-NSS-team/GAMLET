{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/eegorova/.conda/envs/sat/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric import datasets\n",
    "import torch_geometric.utils as utils\n",
    "from sat.models import GraphTransformer, GraphTransformerEncoder\n",
    "from sat.data import GraphDataset\n",
    "from sat.utils import count_parameters\n",
    "from sat.position_encoding import POSENCODINGS\n",
    "from sat.gnn_layers import GNN_TYPES\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, lr_scheduler, epoch, use_cuda=False):\n",
    "    global WARMUP, ABS_PE\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    tic = timer()\n",
    "    for i, data in enumerate(loader):\n",
    "        #print(data)\n",
    "        size = len(data.y)\n",
    "        if WARMUP is not None:\n",
    "            iteration = epoch * len(loader) + i\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr_scheduler(iteration)\n",
    "        if ABS_PE == 'lap':\n",
    "            # sign flip as in Bresson et al. for laplacian PE\n",
    "            sign_flip = torch.rand(data.abs_pe.shape[-1])\n",
    "            sign_flip[sign_flip >= 0.5] = 1.0\n",
    "            sign_flip[sign_flip < 0.5] = -1.0\n",
    "            data.abs_pe = data.abs_pe * sign_flip.unsqueeze(0)\n",
    "\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * size\n",
    "\n",
    "    toc = timer()\n",
    "    n_sample = len(loader.dataset)\n",
    "    epoch_loss = running_loss / n_sample\n",
    "    print('Train loss: {:.4f} time: {:.2f}s'.format(\n",
    "          epoch_loss, toc - tic))\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion, use_cuda=False, split='Val'):\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    mae_loss = 0.0\n",
    "    mse_loss = 0.0\n",
    "\n",
    "    tic = timer()\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            size = len(data.y)\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "\n",
    "            output, _ = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "            mse_loss += F.mse_loss(output, data.y).item() * size\n",
    "            mae_loss += F.l1_loss(output, data.y).item() * size\n",
    "\n",
    "            running_loss += loss.item() * size\n",
    "    toc = timer()\n",
    "\n",
    "    n_sample = len(loader.dataset)\n",
    "    epoch_loss = running_loss / n_sample\n",
    "    epoch_mae = mae_loss / n_sample\n",
    "    epoch_mse = mse_loss / n_sample\n",
    "    print('{} loss: {:.4f} MSE loss: {:.4f} MAE loss: {:.4f} time: {:.2f}s'.format(\n",
    "          split, epoch_loss, epoch_mse, epoch_mae, toc - tic))\n",
    "    return epoch_mae, epoch_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "K_HOP = 2\n",
    "SE = \"gnn\"\n",
    "USE_EDGE_ATTR = True\n",
    "BATCH_SIZE = 128\n",
    "DIM_HIDDEN = 64\n",
    "DROPOUT = 0.2\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "ABS_PE = None\n",
    "ABS_PE_DIM = 20\n",
    "GNN_TYPE = 'graphsage'\n",
    "EDGE_DIM = 32\n",
    "GLOBAL_POOL = 'mean'\n",
    "LAYER_NORM = True\n",
    "BATCH_NORM = not LAYER_NORM\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 1e-5\n",
    "WARMUP = 5000\n",
    "EPOCHS = 10\n",
    "USE_CUDA = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = pd.read_csv('X_dataset.csv').drop(columns='Unnamed: 0')\n",
    "X_dataset = X_dataset.fillna(-1)\n",
    "scaler = StandardScaler()\n",
    "X_dataset = scaler.fit_transform(X_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/eegorova/.local/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "with open('pipelines_graphs/pipeline_graph_rename.pickle', 'rb') as file:\n",
    "    pipeline_graph_rename = pickle.load(file)\n",
    "with open('pipelines_graphs/y.pickle', 'rb') as file:\n",
    "    y_pipeline = list(pickle.load(file))\n",
    "with open('pipelines_graphs/labels.pickle', 'rb') as file:\n",
    "    labels = list(pickle.load(file))\n",
    "with open('pipelines_graphs/pipelines.pickle', 'rb') as file:\n",
    "    pipelines = list(pickle.load(file))\n",
    "\n",
    "pyg_graph = []\n",
    "p = []\n",
    "for idx,graph in enumerate(pipeline_graph_rename):\n",
    "    graph = from_networkx(graph)\n",
    "    graph.y = int(labels[idx])\n",
    "    graph.d = X_dataset[idx]\n",
    "    if graph.edge_index.size(1) != 0:\n",
    "        pyg_graph.append(graph)\n",
    "        p.append(pipelines[idx])\n",
    "    \n",
    "train_dset, test_dset = train_test_split(pyg_graph, test_size=0.7, random_state=SEED)\n",
    "val_dset, test_dset = train_test_split(test_dset, test_size=0.5, random_state=SEED)\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "for dset in pyg_graph:\n",
    "    for item in list(dset.x):\n",
    "        xs.append(int(item))\n",
    "n_tags = len(set(xs))\n",
    "num_edge_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10, LR 0.001000\n",
      "Train loss: 0.7251 time: 21.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1435878/1460692310.py:78: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mse_loss += F.mse_loss(output, data.y).item() * size\n",
      "/tmp/ipykernel_1435878/1460692310.py:79: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss += F.l1_loss(output, data.y).item() * size\n",
      "/tmp/ipykernel_1435878/1460692310.py:78: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mse_loss += F.mse_loss(output, data.y).item() * size\n",
      "/tmp/ipykernel_1435878/1460692310.py:79: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss += F.l1_loss(output, data.y).item() * size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.7404 MSE loss: 0.7590 MAE loss: 0.7404 time: 5.09s\n",
      "Test loss: 0.7307 MSE loss: 0.7452 MAE loss: 0.7307 time: 4.47s\n",
      "Epoch 2/10, LR 0.000004\n",
      "Train loss: 0.7008 time: 21.34s\n",
      "Val loss: 0.7001 MSE loss: 0.6854 MAE loss: 0.7001 time: 4.50s\n",
      "Test loss: 0.6913 MSE loss: 0.6735 MAE loss: 0.6913 time: 4.50s\n",
      "Epoch 3/10, LR 0.000007\n",
      "Train loss: 0.6622 time: 19.86s\n",
      "Val loss: 0.6517 MSE loss: 0.5905 MAE loss: 0.6517 time: 5.56s\n",
      "Test loss: 0.6434 MSE loss: 0.5806 MAE loss: 0.6434 time: 4.40s\n",
      "Epoch 4/10, LR 0.000010\n",
      "Train loss: 0.6192 time: 18.22s\n",
      "Val loss: 0.6063 MSE loss: 0.4946 MAE loss: 0.6063 time: 4.66s\n",
      "Test loss: 0.5989 MSE loss: 0.4867 MAE loss: 0.5989 time: 4.16s\n",
      "Epoch 5/10, LR 0.000013\n",
      "Train loss: 0.5811 time: 20.19s\n",
      "Val loss: 0.5689 MSE loss: 0.4089 MAE loss: 0.5689 time: 4.98s\n",
      "Test loss: 0.5634 MSE loss: 0.4034 MAE loss: 0.5634 time: 4.20s\n",
      "Epoch 6/10, LR 0.000016\n",
      "Train loss: 0.5530 time: 20.48s\n",
      "Val loss: 0.5368 MSE loss: 0.3397 MAE loss: 0.5368 time: 4.62s\n",
      "Test loss: 0.5337 MSE loss: 0.3368 MAE loss: 0.5337 time: 4.24s\n",
      "Epoch 7/10, LR 0.000019\n",
      "Train loss: 0.5263 time: 18.91s\n",
      "Val loss: 0.5021 MSE loss: 0.2909 MAE loss: 0.5021 time: 4.71s\n",
      "Test loss: 0.5016 MSE loss: 0.2908 MAE loss: 0.5016 time: 5.32s\n",
      "Epoch 8/10, LR 0.000022\n",
      "Train loss: 0.4964 time: 19.16s\n",
      "Val loss: 0.4591 MSE loss: 0.2702 MAE loss: 0.4591 time: 5.36s\n",
      "Test loss: 0.4622 MSE loss: 0.2741 MAE loss: 0.4622 time: 4.87s\n",
      "Epoch 9/10, LR 0.000025\n",
      "Train loss: 0.4569 time: 19.58s\n",
      "Val loss: 0.4186 MSE loss: 0.3041 MAE loss: 0.4186 time: 4.31s\n",
      "Test loss: 0.4260 MSE loss: 0.3123 MAE loss: 0.4260 time: 4.12s\n",
      "Epoch 10/10, LR 0.000028\n",
      "Train loss: 0.4275 time: 21.21s\n",
      "Val loss: 0.4190 MSE loss: 0.3577 MAE loss: 0.4190 time: 4.53s\n",
      "Test loss: 0.4284 MSE loss: 0.3689 MAE loss: 0.4284 time: 4.10s\n",
      "best epoch: 8 best val loss: 0.4186\n",
      "\n",
      "Testing...\n",
      "Test loss: 0.4260 MSE loss: 0.3123 MAE loss: 0.4260 time: 4.12s\n",
      "test MAE loss 0.4260\n"
     ]
    }
   ],
   "source": [
    "abs_pe_encoder = None\n",
    "# if args.abs_pe and args.abs_pe_dim > 0:\n",
    "#     abs_pe_method = POSENCODINGS[args.abs_pe]\n",
    "#     abs_pe_encoder = abs_pe_method(args.abs_pe_dim, normalization='sym')\n",
    "#     if abs_pe_encoder is not None:\n",
    "#         abs_pe_encoder.apply_to(train_dset)\n",
    "#         abs_pe_encoder.apply_to(val_dset)\n",
    "\n",
    "deg = torch.cat([\n",
    "    utils.degree(data.edge_index[1], num_nodes=data.num_nodes) for\n",
    "    data in train_dset])\n",
    "\n",
    "model = GraphTransformer(in_size=n_tags,\n",
    "                            num_class=1,\n",
    "                            d_model=DIM_HIDDEN,\n",
    "                            n_dataset = 10,\n",
    "                            dim_feedforward=2*DIM_HIDDEN,\n",
    "                            dropout=DROPOUT,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            batch_norm=BATCH_NORM,\n",
    "                            abs_pe=ABS_PE,\n",
    "                            abs_pe_dim=ABS_PE_DIM,\n",
    "                            gnn_type=GNN_TYPE,\n",
    "                            use_edge_attr=USE_EDGE_ATTR,\n",
    "                            num_edge_features=num_edge_features,\n",
    "                            edge_dim=EDGE_DIM,\n",
    "                            k_hop=K_HOP,\n",
    "                            se=SE,\n",
    "                            deg=deg,\n",
    "                            global_pool=GLOBAL_POOL) \n",
    "\n",
    "# if args.use_cuda:\n",
    "#     model.cuda()\n",
    "# print(\"Total number of parameters: {}\".format(count_parameters(model)))\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "if WARMUP is None:\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                        factor=0.5,\n",
    "                                                        patience=15,\n",
    "                                                        min_lr=1e-05,\n",
    "                                                        verbose=False)\n",
    "else:\n",
    "    lr_steps = (LR - 1e-6) / WARMUP\n",
    "    decay_factor = LR * WARMUP ** .5\n",
    "    def lr_scheduler(s):\n",
    "        if s < WARMUP:\n",
    "            lr = 1e-6 + s * lr_steps\n",
    "        else:\n",
    "            lr = decay_factor * s ** -.5\n",
    "        return lr\n",
    "\n",
    "\n",
    "\n",
    "#FIXME\n",
    "if abs_pe_encoder is not None:\n",
    "    abs_pe_encoder.apply_to(test_dset)\n",
    "\n",
    "print(\"Training...\")\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_epoch = 0\n",
    "logs = defaultdict(list)\n",
    "start_time = timer()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch {}/{}, LR {:.6f}\".format(epoch + 1, EPOCHS, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, lr_scheduler, epoch, USE_CUDA)\n",
    "    val_loss,_ = eval_epoch(model, val_loader, criterion, USE_CUDA, split='Val')\n",
    "    test_loss,_ = eval_epoch(model, test_loader, criterion, USE_CUDA, split='Test')\n",
    "\n",
    "    if WARMUP is None:\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "    logs['train_mae'].append(train_loss)\n",
    "    logs['val_mae'].append(val_loss)\n",
    "    logs['test_mae'].append(test_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "total_time = timer() - start_time\n",
    "print(\"best epoch: {} best val loss: {:.4f}\".format(best_epoch, best_val_loss))\n",
    "model.load_state_dict(best_weights)\n",
    "\n",
    "print()\n",
    "print(\"Testing...\")\n",
    "test_loss, test_mse_loss = eval_epoch(model, test_loader, criterion, USE_CUDA, split='Test')\n",
    "\n",
    "print(\"test MAE loss {:.4f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.DataFrame.from_dict(logs)\n",
    "logs.to_csv('models/logs.csv')\n",
    "\n",
    "torch.save(\n",
    "    {#'args': args,\n",
    "    'state_dict': best_weights},\n",
    "    'models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embs(model, loader, use_cuda=False, split='Val'):\n",
    "    model.eval()\n",
    "\n",
    "    emds = []\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "\n",
    "            res, emb = model(data)\n",
    "            for e,r in zip(emb,res):\n",
    "                emds.append(e)\n",
    "                result.append(r)\n",
    "\n",
    "    return result, emds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loader = DataLoader(pyg_graph, batch_size=BATCH_SIZE, shuffle=False)\n",
    "res, embs = get_embs(model, graph_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(graph_loader)\n",
    "data = next(dataiter)\n",
    "res, emb = model(data)\n",
    "# torch.max(res, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipelines_graphs/emb_pipelines_100epochs_label.pickle', 'wb') as file:\n",
    "    pickle.dump(embs, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('sat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "794105dfe45acd45667c0ec848c3c17bdbc409cda564f877ba431899b91ef0e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
