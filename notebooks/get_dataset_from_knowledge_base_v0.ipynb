{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:50:55.756997846Z",
     "start_time": "2023-05-17T18:50:55.695067100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run this notebook from `<repository_root>/notebooks`'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Run this notebook from `<repository_root>/notebooks`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:08.440588648Z",
     "start_time": "2023-05-17T18:51:07.273732419Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import openml\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from meta_automl.data_preparation.models_loaders import KnowledgeBaseModelsLoader\n",
    "from meta_automl.data_preparation.pipeline_features_extractors import FEDOTPipelineFeaturesExtractor\n",
    "from meta_automl.data_preparation.meta_features_extractors import OpenMLDatasetMetaFeaturesExtractor\n",
    "from meta_automl.data_preparation.feature_preprocessors import FeaturesPreprocessor\n",
    "from fedot.core.pipelines.pipeline import Pipeline\n",
    "from torch_geometric.data import Data\n",
    "from meta_automl.data_preparation.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseToDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        knowledge_base_directory: str,\n",
    "        dataset_directory: str,\n",
    "        meta_features_data_columns: List[str],\n",
    "        split: Optional[str] = \"all\", # Can be train, test, all\n",
    "        train_test_split_name: Optional[str] = \"train_test_datasets_classification.csv\",\n",
    "        task_type: Optional[str] = \"classification\",\n",
    "        fitness_metric: Optional[str] = \"f1\",\n",
    "        exclude_datasets: Optional[List[str]] = None,  # In accordance to @MorrisNein message in chat\n",
    "        meta_features_preprocessors: Dict[str, Any] = None,\n",
    "    ) -> None:\n",
    "        if task_type != \"classification\":\n",
    "            raise NotImplementedError(f\"Current version if for `'classification'` `task_type`\")\n",
    "        if fitness_metric == \"log_loss\":\n",
    "            self.fitness_coef = 1\n",
    "        else:\n",
    "            self.fitness_coef = -1\n",
    "        \n",
    "        self.knowledge_base_directory = knowledge_base_directory\n",
    "        self.dataset_directory = dataset_directory\n",
    "        self.meta_features_data_columns = meta_features_data_columns\n",
    "        self.train_test_split_name = train_test_split_name\n",
    "        self.task_type = task_type\n",
    "        self.split = split\n",
    "        self.fitness_metric = fitness_metric\n",
    "        self.exclude_datasets = exclude_datasets\n",
    "        self.meta_features_preprocessors = meta_features_preprocessors\n",
    "\n",
    "        self._maybe_create_dataset_directory(os.path.join(self.dataset_directory, self.split))\n",
    "\n",
    "        self.pipeline_extractor = FEDOTPipelineFeaturesExtractor()\n",
    "        self.meta_features_extractor = OpenMLDatasetMetaFeaturesExtractor(meta_features_data_columns=self.meta_features_data_columns)\n",
    "\n",
    "        self.models_loader = KnowledgeBaseModelsLoader(self.knowledge_base_directory)\n",
    "        df_datasets = self.models_loader.parse_datasets(self.split, self.task_type)\n",
    "        self.df_datasets = df_datasets[df_datasets[\"dataset_name\"].apply(lambda x: x not in self.exclude_datasets)]\n",
    "        self._check_for_duplicated_datasets()\n",
    "\n",
    "    def _check_for_duplicated_datasets(self):\n",
    "        occurences = self.df_datasets.dataset_id.value_counts()\n",
    "        unique_number_of_occurences = set(occurences.to_list())\n",
    "        assert len(unique_number_of_occurences) == 1, f\"Duplicated datasets detected. Check datasets: \\n{occurences}\"\n",
    "        assert unique_number_of_occurences.pop() == 1, f\"Duplicated datasets detected. Check datasets: \\n{occurences}\"\n",
    "\n",
    "    def _maybe_create_dataset_directory(self, directory: str) -> None:\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "\n",
    "    def _get_pipeline_features(self, pipeline: Pipeline) -> Data:\n",
    "        pipeline_json_string = pipeline.save()[0].encode()\n",
    "        return self.pipeline_extractor(pipeline_json_string)\n",
    "\n",
    "    def _get_best_pipelines_unique_indexes(self, dataset_models: List[Model]) -> List[int]:\n",
    "        temp_df = pd.DataFrame(columns=[\"predictor\", \"fitness\"])\n",
    "        temp_df[\"predictor\"] = [str(x.predictor) for x in dataset_models]\n",
    "        temp_df[\"fitness\"] = [self.fitness_coef * x.fitness.value for x in dataset_models]\n",
    "        # Select top-1 pipeline \n",
    "        best_pipelines_unique_indexes = temp_df.groupby('predictor')['fitness'].idxmax().to_list()\n",
    "        return best_pipelines_unique_indexes\n",
    "    \n",
    "    def _process(self) -> Tuple[List[Dict[str, Union[float, int]]], List[Dict[str, float]], List[Data], List[int]]:\n",
    "        pipeline_id = 0\n",
    "\n",
    "        task_pipe_comb = []\n",
    "        datasets_meta_features = []\n",
    "        pipelines = []\n",
    "        is_train_flags = []\n",
    "        \n",
    "        for task_id in tqdm(self.df_datasets.index):\n",
    "            dataset = self.df_datasets.loc[task_id]\n",
    "            datasets_meta_features.append(self.meta_features_extractor(dataset.dataset_id))\n",
    "            is_train_flags.append(dataset.is_train)\n",
    "\n",
    "            dataset_models = self.models_loader.load(\n",
    "                dataset_names=[dataset.dataset_name],\n",
    "                fitness_metric=self.fitness_metric,\n",
    "            )\n",
    "            best_pipelines_unique_indexes = self._get_best_pipelines_unique_indexes(dataset_models)\n",
    "            \n",
    "            for index in best_pipelines_unique_indexes:\n",
    "                model = dataset_models[index]\n",
    "                pipelines.append(self._get_pipeline_features(model.predictor))\n",
    "                y = self.fitness_coef * model.fitness.value\n",
    "                task_pipe_comb.append({\"task_id\": task_id, \"pipeline_id\": pipeline_id, \"y\": y})\n",
    "                pipeline_id += 1\n",
    "\n",
    "        return task_pipe_comb, datasets_meta_features, pipelines, is_train_flags\n",
    "    \n",
    "    def _save_task_pipe_comb(self, task_pipe_comb: List[Dict[str, Union[float, int]]]):\n",
    "        task_pipe_comb_df = pd.DataFrame.from_records(task_pipe_comb)\n",
    "        task_pipe_comb_df.to_csv(\n",
    "            os.path.join(self.dataset_directory, self.split, \"task_pipe_comb.csv\"),\n",
    "            header=True,\n",
    "            index=True,\n",
    "        )\n",
    "    \n",
    "    def _save_datasets_meta_features(self, datasets_meta_features: List[Dict[str, float]]):\n",
    "        df = pd.DataFrame.from_records(datasets_meta_features)\n",
    "        if self.meta_features_preprocessors is not None:\n",
    "            df_as_dict = {k: list(v.values()) for k, v in df.to_dict().items()}\n",
    "            self.meta_features_preprocessors.fit(\n",
    "                df_as_dict, \n",
    "                os.path.join(self.dataset_directory, self.split, \"meta_features_preprocessors.pickle\"),\n",
    "            )\n",
    "            transformed = self.meta_features_preprocessors.transform(df_as_dict, single=False)\n",
    "            df = pd.DataFrame.from_dict({k: v.reshape(-1) for k,v in transformed.items()})\n",
    "        \n",
    "        df.to_csv(\n",
    "            os.path.join(self.dataset_directory, self.split, \"datasets.csv\"),\n",
    "            header=True,\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    def _save_pipelines(self, pipelines: List[Data]):\n",
    "        with open(os.path.join(self.dataset_directory, self.split, \"pipelines.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(pipelines, f)\n",
    "    \n",
    "    def _save_split(self, is_train_flags: List[int]):\n",
    "        split = {\n",
    "            \"train\": [],\n",
    "            \"test\": [],\n",
    "        }\n",
    "        for i, flag in enumerate(is_train_flags):\n",
    "            if flag == 1:\n",
    "                split[\"train\"].append(i)\n",
    "            else:\n",
    "                split[\"test\"].append(i)\n",
    "        with open(os.path.join(self.dataset_directory, self.split, \"split.json\"), \"w\") as f:\n",
    "            json.dump(split, f)\n",
    "\n",
    "    \n",
    "    def convert(self):\n",
    "        task_pipe_comb, datasets_meta_features, pipelines, is_train_flags = self._process()\n",
    "        self._save_split(is_train_flags)\n",
    "        self._save_pipelines(pipelines)\n",
    "        self._save_datasets_meta_features(datasets_meta_features)\n",
    "        self._save_task_pipe_comb(task_pipe_comb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The identity link alias is deprecated. Use Identity instead. The identity link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The inverse_power link alias is deprecated. Use InversePower instead. The inverse_power link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "/home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The inverse_squared link alias is deprecated. Use InverseSquared instead. The inverse_squared link alias will be removed after the 0.15.0 release.\n",
      "  warnings.warn(\n",
      "2023-07-15 00:14:44.407228: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-15 00:14:45.140209: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-07-15 00:14:45.140280: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cherniak/.local/share/virtualenvs/MetaFEDOT-wsJWSqtd/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-07-15 00:14:45.140286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 41/41 [00:23<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessors = {\n",
    "    \"MajorityClassSize\": StandardScaler(),\n",
    "    \"MaxNominalAttDistinctValues\": StandardScaler(),\n",
    "    \"MinorityClassSize\": StandardScaler(),\n",
    "    \"NumberOfClasses\": StandardScaler(),\n",
    "    \"NumberOfFeatures\": StandardScaler(),\n",
    "    \"NumberOfInstances\": StandardScaler(),\n",
    "    \"NumberOfInstancesWithMissingValues\": StandardScaler(),\n",
    "    \"NumberOfMissingValues\": StandardScaler(),\n",
    "    \"NumberOfNumericFeatures\": StandardScaler(),\n",
    "    \"NumberOfSymbolicFeatures\": StandardScaler(),\n",
    "}\n",
    "meta_features_preprocessor = FeaturesPreprocessor(preprocessors)\n",
    "\n",
    "\n",
    "converter = KnowledgeBaseToDataset(\n",
    "    knowledge_base_directory = \"../data/knowledge_base_0\",\n",
    "    dataset_directory = \"../data/openml_meta_features_and_fedot_pipelines\",\n",
    "    meta_features_data_columns = [\n",
    "        \"MajorityClassSize\",\n",
    "        \"MaxNominalAttDistinctValues\",\n",
    "        \"MinorityClassSize\",\n",
    "        \"NumberOfClasses\",\n",
    "        \"NumberOfFeatures\",\n",
    "        \"NumberOfInstances\",\n",
    "        \"NumberOfInstancesWithMissingValues\",\n",
    "        \"NumberOfMissingValues\",\n",
    "        \"NumberOfNumericFeatures\",\n",
    "        \"NumberOfSymbolicFeatures\"\n",
    "    ],\n",
    "    train_test_split_name = \"train_test_datasets_classification.csv\",\n",
    "    task_type=\"classification\",\n",
    "    fitness_metric = \"f1\",\n",
    "    exclude_datasets = [\"connect-4\", \"higgs\"],\n",
    "    meta_features_preprocessors=meta_features_preprocessor,\n",
    ")\n",
    "converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MetaFEDOT-wsJWSqtd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
