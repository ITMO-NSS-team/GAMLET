{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:50:55.756997846Z",
     "start_time": "2023-05-17T18:50:55.695067100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run this notebook from `<repository_root>/notebooks`'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Run this notebook from `<repository_root>/notebooks`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:08.440588648Z",
     "start_time": "2023-05-17T18:51:07.273732419Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import openml\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from meta_automl.data_preparation.models_loaders import KnowledgeBaseModelsLoader\n",
    "from surrogate.datasets import HomogeneousPipelineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:09.594042006Z",
     "start_time": "2023-05-17T18:51:09.584377707Z"
    }
   },
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE_DIRECTORY_PATH = \"../data/knowledge_base_0\"\n",
    "TRAIN_TEST_SPLIT_PATH = \"../data/knowledge_base_0/train_test_datasets_classification.csv\"\n",
    "\n",
    "DATASET_DIRNAME = \"openml_meta_features_and_fedot_pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:10.083108023Z",
     "start_time": "2023-05-17T18:51:10.075805147Z"
    }
   },
   "outputs": [],
   "source": [
    "EXCLUDE_KNOWLEDGE_BASE_DATASETS = [\n",
    "    \"connect-4\",\n",
    "    \"higgs\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:11.020051438Z",
     "start_time": "2023-05-17T18:51:11.006218208Z"
    }
   },
   "outputs": [],
   "source": [
    "FITNESS_METRIC = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:11.401493924Z",
     "start_time": "2023-05-17T18:51:11.393589989Z"
    }
   },
   "outputs": [],
   "source": [
    "TASK_TYPE = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:12.072515885Z",
     "start_time": "2023-05-17T18:51:12.060055257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make dataset directory\n",
    "\n",
    "if not os.path.exists(os.path.join(\"../data\", DATASET_DIRNAME)):\n",
    "    os.mkdir(os.path.join(\"../data\", DATASET_DIRNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:12.477691683Z",
     "start_time": "2023-05-17T18:51:12.467643254Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df_meta_features(df_datasets: pd.DataFrame):\n",
    "    df_meta_features = openml.datasets.list_datasets(\n",
    "        df_datasets[\"dataset_id\"], \n",
    "        output_format=\"dataframe\",\n",
    "    )\n",
    "    df_meta_features.reset_index(drop=True, inplace=True)\n",
    "    df_meta_features.fillna(-1, inplace=True)\n",
    "    return df_meta_features\n",
    "\n",
    "def get_df_meta_features_fitted_scaller(knowledge_base_path: str, scaler_class, split:str, task_type: str, data_columns):\n",
    "\n",
    "    models_loader = KnowledgeBaseModelsLoader(knowledge_base_path)\n",
    "    df_datasets = models_loader.parse_datasets(split, task_type)\n",
    "    df_meta_features = get_df_meta_features(df_datasets)\n",
    "    \n",
    "    scaler = scaler_class()\n",
    "    scaler.fit(df_meta_features[data_columns])\n",
    "\n",
    "    df_meta_features[data_columns] = scaler.transform(df_meta_features[data_columns])\n",
    "    return scaler\n",
    "\n",
    "def get_preprocessed_meta_features(df_datasets: pd.DataFrame, scaler, data_columns: List[str]):\n",
    "    df_meta_features = get_df_meta_features(df_datasets)\n",
    "    df_meta_features[data_columns] = scaler.transform(df_meta_features[data_columns])\n",
    "    return df_meta_features\n",
    "\n",
    "def get_dataset(\n",
    "        knowledge_base_path: str, \n",
    "        scaler, \n",
    "        split: str, \n",
    "        dataset_dir_path: str,\n",
    "        meta_features_data_columns: List[str],\n",
    "        fitness_metric: str = \"f1\", \n",
    "        task_type: str = \"classification\",\n",
    "        exclude_datasets_names: List[\"str\"] = [],\n",
    "    ):\n",
    "    \"\"\"\n",
    "    split: `train`/`test`.\n",
    "    task_type: `classification`, `regression`, `ts_forecasting`.\n",
    "    \"\"\"\n",
    "    if task_type != \"classification\":\n",
    "        raise NotImplementedError(f\"Check if this implementation is suitable for {task_type}\")\n",
    "    if fitness_metric == \"log_loss\":\n",
    "        fitness_coef = 1\n",
    "    else:\n",
    "        fitness_coef = -1\n",
    "    \n",
    "    if not os.path.exists(os.path.join(\"../data\", dataset_dir_path, split)):\n",
    "        os.mkdir(os.path.join(\"../data\", dataset_dir_path, split))\n",
    "\n",
    "    temp_pipelines_dir = \"./temp_pipelines_dir\"\n",
    "    # if os.path.exists(temp_pipelines_dir):\n",
    "    #     shutil.rmtree(temp_pipelines_dir)\n",
    "    \n",
    "    models_loader = KnowledgeBaseModelsLoader(knowledge_base_path)\n",
    "    \n",
    "    df_datasets = models_loader.parse_datasets(split, task_type)\n",
    "\n",
    "    df_meta_features = get_preprocessed_meta_features(df_datasets, scaler, meta_features_data_columns)\n",
    "    df_meta_features.to_csv(\n",
    "        os.path.join(\"../data\", dataset_dir_path, split, \"meta_features.csv\"), \n",
    "        columns=meta_features_data_columns,\n",
    "        header=False,\n",
    "        index=False)\n",
    "\n",
    "    pipeline_id = 0\n",
    "    records = []\n",
    "    for _, dataset in df_datasets.iterrows():\n",
    "        if dataset[\"dataset_name\"] in exclude_datasets_names:\n",
    "            continue\n",
    "\n",
    "        if len(df_meta_features[df_meta_features[\"did\"] == dataset[\"dataset_id\"]]) > 1:\n",
    "            raise ValueError(f\"Dataset id is not unique: {dataset['dataset_id']}\")\n",
    "        task_id = df_meta_features[df_meta_features[\"did\"] == dataset[\"dataset_id\"]].index[0]\n",
    "\n",
    "        dataset_name = dataset[\"dataset_name\"]\n",
    "        \n",
    "        dataset_models = models_loader.load(\n",
    "            dataset_names=[dataset_name],  # load models just for this exact dataset.\n",
    "            fitness_metric=fitness_metric,  # must correspond to a metric name in a knowledge base.\n",
    "        )\n",
    "        data = [(str(x.predictor), fitness_coef * x.fitness.value) for x in dataset_models]\n",
    "\n",
    "        temp_df = pd.DataFrame(data=data, columns=[\"predictor\", \"fitness\"])\n",
    "        best_unique_pipelines_indexes = temp_df.groupby('predictor')['fitness'].idxmax().to_list()\n",
    "        \n",
    "        for index in best_unique_pipelines_indexes:\n",
    "            model = dataset_models[index]\n",
    "            y = fitness_coef * model.fitness.value\n",
    "\n",
    "            model.predictor.save(os.path.join(temp_pipelines_dir, f\"pipeline_{pipeline_id}.json\"))\n",
    "\n",
    "            records.append({\"task_id\": task_id, \"pipeline_id\":pipeline_id, \"y\": y})\n",
    "            \n",
    "            pipeline_id += 1\n",
    "\n",
    "    task_pipe_comb_df = pd.DataFrame.from_records(records)\n",
    "    task_pipe_comb_df.to_csv(\n",
    "        os.path.join(\"../data\", dataset_dir_path, split, \"task_pipe_comb_df.csv\"), \n",
    "        header=True,\n",
    "        index=True,\n",
    "    )\n",
    "\n",
    "    dataset = HomogeneousPipelineDataset(\n",
    "        root=temp_pipelines_dir,\n",
    "        direction=\"directed\",\n",
    "        use_operations_hyperparameters=False,\n",
    "        overriden_processed_dir=None #overriden_processed_dir=\"./processed\"\n",
    "    )\n",
    "    return dataset\n",
    "#     shutil.rmtree(temp_pipelines_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T18:51:13.231059718Z",
     "start_time": "2023-05-17T18:51:13.219889419Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_features_data_columns = [\n",
    "    \"MajorityClassSize\",\n",
    "    \"MaxNominalAttDistinctValues\",\n",
    "    \"MinorityClassSize\",\n",
    "    \"NumberOfClasses\",\n",
    "    \"NumberOfFeatures\",\n",
    "    \"NumberOfInstances\",\n",
    "    \"NumberOfInstancesWithMissingValues\",\n",
    "    \"NumberOfMissingValues\",\n",
    "    \"NumberOfNumericFeatures\",\n",
    "    \"NumberOfSymbolicFeatures\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_scaller = get_df_meta_features_fitted_scaller(\n",
    "    KNOWLEDGE_BASE_DIRECTORY_PATH, \n",
    "    StandardScaler, \n",
    "    split=\"all\",\n",
    "    task_type=TASK_TYPE,\n",
    "    data_columns=meta_features_data_columns,\n",
    ")\n",
    "\n",
    "dset = get_dataset(\n",
    "    KNOWLEDGE_BASE_DIRECTORY_PATH,\n",
    "    scaler=meta_features_scaller,\n",
    "    split=\"train\",\n",
    "    dataset_dir_path=DATASET_DIRNAME,\n",
    "    meta_features_data_columns=meta_features_data_columns,\n",
    "    fitness_metric=FITNESS_METRIC,\n",
    "    task_type=TASK_TYPE,\n",
    "    exclude_datasets_names=EXCLUDE_KNOWLEDGE_BASE_DATASETS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-parse]",
   "language": "python",
   "name": "conda-env-.conda-parse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
